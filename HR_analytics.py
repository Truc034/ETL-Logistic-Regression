# -*- coding: utf-8 -*-
"""Session 5.7: ETL & Logistic Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVTQfQeRcCJqPZ8LLlIkpY0-W3NkH3NG

# 0. Import libaries

First, import necessary libraries.
"""

import pandas as pd

"""Then install and import the MySQL connector for Python:"""

!pip install pymysql

"""Let's import this:"""

from sqlalchemy import create_engine
import pymysql

import requests

"""# 1. Extract Data

Data is collected from various sources including an Excel file, a CSV file, a Google Sheet, a web page, and an SQL database.

## 1.1. Enrollies' data

Extract data from Google Sheets:

https://docs.google.com/spreadsheets/d/1VCkHwBjJGRJ21asd9pxW4_0z2PWuKhbLR3gUHm-p4GI/edit?usp=sharing
"""

# take the Google Sheet file ID:
google_sheet_id = '1VCkHwBjJGRJ21asd9pxW4_0z2PWuKhbLR3gUHm-p4GI'

# retrieve file as XLSX:
url = 'https://docs.google.com/spreadsheets/d/' + google_sheet_id + '/export?format=xlsx'

# retrieve it as a regular Excel file:
enrollies_data = pd.read_excel(url, sheet_name='enrollies')

""" Take a look at the given data:"""

enrollies_data.head()

"""##1.2. Enrollies' education

Educational department stores it in the Excel format here:

https://assets.swisscoding.edu.vn/company_course/enrollies_education.xlsx
"""

# download and parse excel by the link
excel_url = 'https://assets.swisscoding.edu.vn/company_course/enrollies_education.xlsx'
excel_respond = requests.get(excel_url)
with open('enrollies_education.xlsx', 'wb') as file:
    file.write(excel_respond.content)

# Read data from Excel file
enrollies_education = pd.read_excel('enrollies_education.xlsx', sheet_name='enrollies_education')

"""Take a look at the given data:"""

enrollies_education.head()

"""##1.3. Enrollies' working experience

Educational department stores it in the CSV format here:

https://assets.swisscoding.edu.vn/company_course/work_experience.csv
"""

# download and parse csv by the link
csv_url = 'https://assets.swisscoding.edu.vn/company_course/work_experience.csv'
csv_respond = requests.get(csv_url)
with open('work_experience.csv', 'wb') as file:
    file.write(csv_respond.content)

# Read data from CSV file
work_experience = pd.read_csv('work_experience.csv')

"""Take a look at the given data:"""

work_experience.head()

"""##1.4. Training hours

From LMS system's database you can retrieve a number of training hours for each student that they have completed.

**Database credentials:**

- Database type: `MySQL`
- Host: `112.213.86.31`
- Port: `3360`
- Login: `etl_practice`
- Password: `550814`
- Database name: `company_course`
- Table name: `training_hours`

Connect to the database via SQL Alchemy engine and save the connection into a variable using a URL in the following format:

`<driver>://<login>:<password>@<host>:<port>/<database_name>`
"""

# Connect to the database
engine = create_engine('mysql+pymysql://etl_practice:550814@112.213.86.31:3360/company_course')

"""Then just retrieve the data using `.read_sql_table()` method:"""

# Load data
training_hours = pd.read_sql_table('training_hours', engine)

"""Take a look at the given data:"""

training_hours.head()

"""##1.5. City development index

Data stored here:

https://sca-programming-school.github.io/city_development_index/index.html
"""

# Read data from HTML
tables = pd.read_html('https://sca-programming-school.github.io/city_development_index/index.html')

# Load data from the first table in website
cities = tables[0]

"""Take a look at the given data:"""

cities.head()

"""## 1.6. Employment

From LMS database you can also retrieve the fact of employment. If student is marked as employed, it means that this student started to work in our company after finishing the course.

**Database credentials:**

- Database type: `MySQL`
- Host: `112.213.86.31`
- Port: `3360`
- Login: `etl_practice`
- Password: `550814`
- Database name: `company_course`
- Table name: `employment`

Retrieve the data:
"""

# Load data
employment = pd.read_sql_table('employment', engine)

"""Take a look at the given data:"""

employment.head()

"""# 2. Transform Data

Data is transformed through multiple steps including cleaning, formatting, normalization, and feature creation to ensure consistency across sources.

## 2.1. Enrollies' data

Display basic information about the dataset:
"""

enrollies_data.info()

"""a. Fix data type:"""

# Convert columns to appropriate data types
type_mappings = {
    'full_name': 'string',
    'city': 'category',
    'gender': 'category'
}

for col, dtype in type_mappings.items():
    enrollies_data[col] = enrollies_data[col].astype(dtype)

"""b. Fill missing value:"""

# find mode value
gender_mode = enrollies_data['gender'].mode()[0]

# Fill missing values with the most frequent (mode) value
enrollies_data['gender'].fillna(gender_mode, inplace=True)

"""Let's check a result:"""

enrollies_data.info()

"""## 2.2. Enrollies' education

Display basic information about the dataset:
"""

enrollies_education.info()

"""a. Fix data type:"""

# Convert selected columns to 'category' data type
for col in ['enrolled_university', 'education_level', 'major_discipline']:
    enrollies_education[col] = enrollies_education[col].astype('category')

"""b. Fill missing value:"""

# Fill missing values with mode for selected categorical columns
for col in ['enrolled_university', 'education_level', 'major_discipline']:
    mode_val = enrollies_education[col].mode()[0]
    enrollies_education[col].fillna(mode_val, inplace=True)

"""Let's check a result:"""

enrollies_education.info()

"""## 2.3. Enrollies' working experience

Display basic information about the dataset:
"""

work_experience.info()

"""a. Fix data type:"""

# Convert selected columns to 'category' data type
categorical_cols = ['relevent_experience', 'experience', 'company_size', 'company_type', 'last_new_job']
for col in categorical_cols:
    work_experience[col] = work_experience[col].astype('category')

"""b. Fill missing value:"""

# Fill missing values with mode for selected categorical columns in work_experience
for col in ['experience', 'company_size', 'company_type', 'last_new_job']:
    mode_val = work_experience[col].mode()[0]
    work_experience[col].fillna(mode_val, inplace=True)

"""Let's check a result:"""

work_experience.info()

"""## 2.4. Training hours

Display basic information about the dataset:
"""

training_hours.info()

"""Since the dataset is already clean and complete, no transformation steps were necessary before analysis.

## 2.5. City development index

Display basic information about the dataset:
"""

cities.info()

"""Fix data type:"""

cities['City'] = cities['City'].astype('category')

cities.info()

"""## 2.6. Employment

Display basic information about the dataset:
"""

employment.info()

"""Since the dataset is already clean and complete, no transformation steps were necessary before analysis.

# 3. Load Data

Cleaned and structured data is loaded into a SQLite database using SQLAlchemy for storage and future querying.
"""

# Define the path for the SQLite database
db_path = 'data_warehouse.db'

# Create a SQLAlchemy engine for connecting to the SQLite database
engine = create_engine(f'sqlite:///{db_path}')

"""Load dimension and fact tables into the database:

"""

# Dictionary of DataFrames and corresponding table names
tables_to_load = {
    'dim_enrollies_data': enrollies_data,
    'fact_enrollies_education': enrollies_education,
    'dim_work_experience': work_experience,
    'dim_training_hours': training_hours,
    'dim_cities': cities,
    'dim_employment': employment
}

# Load each DataFrame into the SQLite database
for table_name, df in tables_to_load.items():
    df.to_sql(table_name, engine, if_exists='replace', index=False)